{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 時間情報のない集団データ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方針"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・内部に問題ごとの依存関係を定義し、その関係をもとに遷移させる。\n",
    "\n",
    "・条件付き確率（遷移確率）と周辺分布の積の和から次の周辺分布を求める。\n",
    "\n",
    "・まずは遷移過程が部分的にしか分からない人工データを用意し、モデルを学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人工データ生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 問題の依存関係の行列 A \n",
    "A = np.array([\n",
    "    [0, 0, 0, 0, 0, 0],  # 初期状態\n",
    "    [1, 0, 0, 0, 0, 0],  # 問題1は初期状態のみに依存\n",
    "    [1, 0, 0, 0, 0, 0],  # 問題2は問題1に依存\n",
    "    [0, 1, 1, 0, 0, 0],  # 問題3は問題2に依存\n",
    "    [0, 0, 0, 1, 0, 0],  # 問題4は問題2、問題3に依存\n",
    "    [0, 0, 0, 0, 1, 0]   # 問題4は問題2、問題3に依存\n",
    "], dtype = float)*3\n",
    "\n",
    "# 0でない要素の数で割る処理\n",
    "nonzero_counts = np.count_nonzero(A[1:], axis=1, keepdims=True)\n",
    "A[1:] = np.where(nonzero_counts != 0, A[1:] / nonzero_counts, 0)\n",
    "\n",
    "# 遷移確率を計算する関数\n",
    "def calculate_transition_probabilities(A, X):\n",
    "    n = len(X)\n",
    "    raw_probabilities = np.zeros(n)  # 遷移確率の元となる値\n",
    "    \n",
    "    # 不正解の問題に対して遷移確率を計算\n",
    "    for i in range(n):\n",
    "        if X[i] == 0:  # まだ正解していない問題のみ対象\n",
    "            required_problems = A[i, :]  # i番目の問題に必要な依存関係\n",
    "            \n",
    "            solved_problems = X * required_problems  # 現状解けている\n",
    "            \n",
    "            num_solved = np.sum(solved_problems)      # 実際に解けた問題の数\n",
    "            \n",
    "            raw_probabilities[i] = np.exp(num_solved)\n",
    "    \n",
    "    # 総和で割って正規化\n",
    "    total_sum = np.sum(raw_probabilities)  # expの総和\n",
    "    if total_sum > 0:  # 総和が0でなければ正規化\n",
    "        probabilities = raw_probabilities / total_sum\n",
    "    else:\n",
    "        probabilities = raw_probabilities  # 総和が0ならそのまま\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 教師データセットを生成する関数\n",
    "def generate_training_data(A, initial_X, num_correct_problems, num_data_per_step):\n",
    "    n = len(initial_X)  # 問題数\n",
    "    dataset = []\n",
    "    \n",
    "    # 各ステップでデータを生成\n",
    "    for i in range(num_correct_problems, num_correct_problems+1):  # 1問以上の正解を対象にする\n",
    "        for j in range(num_data_per_step):  # 各ステップごとにデータ数\n",
    "            X = initial_X.copy()  # 初期状態からスタート\n",
    "            \n",
    "            # i問正解させる\n",
    "            for k in range(i):\n",
    "                input_X = X.copy()    # 遷移前状態を保持\n",
    "                probabilities = calculate_transition_probabilities(A, X)\n",
    "                \n",
    "                if np.sum(probabilities) > 0:  # 正規化された確率がある場合\n",
    "                    # 確率に基づいて次に正解させる問題を選択\n",
    "                    next_correct_problem = np.random.choice(n, p=probabilities)\n",
    "                    X[next_correct_problem] = 1  # 選ばれた問題を正解に遷移させる\n",
    "            \n",
    "                # 初期状態と1ステップ後の状態の差分を教師データとして使用\n",
    "                target_Y = (X - input_X).clip(min=0)  # 0から1に変わった部分のみを1、他は0\n",
    "                \n",
    "                # 初期状態（入力）と差分（教師データ）のペアを保存\n",
    "                dataset.append((input_X.copy(), target_Y.copy()))  # (入力データ, 教師データ)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bcsoftmax1d(x, budget):\n",
    "    \"\"\"Budget Constrained Softmax function for vector.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): input vector. shape: (n_outputs, )\n",
    "        budget (Tensor): budget (constraint) vector. shape: (n_outputs, )\n",
    "\n",
    "    Returns:\n",
    "        y (Tensor): output probability vector. shape: (n_outputs, ). Satisfying the constraints y_i <= budget_i.\n",
    "    \n",
    "    \"\"\"\n",
    "    x = x - torch.max(x, dim=0)[0] # normalization to avoid numerical errors\n",
    "    exp_x = torch.exp(x)\n",
    "    # sorting\n",
    "    _, indices = torch.sort(budget / exp_x, descending=False)\n",
    "    exp_x = exp_x[indices]\n",
    "    budget = budget[indices]\n",
    "    # find K_B\n",
    "    r = torch.flip(torch.cumsum(torch.flip(exp_x, dims=(0, )), dim=0), dims=(0, ))\n",
    "    s = 1.0 - (torch.cumsum(budget, dim=0) - budget)\n",
    "    z = r / s\n",
    "    is_in_KB = torch.logical_and(\n",
    "        (s - budget) > 0, exp_x / z > budget\n",
    "    )\n",
    "    # compute outputs\n",
    "    s = 1 - torch.sum(budget * is_in_KB)\n",
    "    r = torch.sum(exp_x * (~is_in_KB))\n",
    "    y = torch.where(~is_in_KB, s * exp_x / r, budget)\n",
    "    # undo sorting\n",
    "    _, inv_indices = torch.sort(indices, descending=False)\n",
    "    return y[inv_indices]\n",
    "\n",
    "\n",
    "def _bcsoftmax1d_stable(x, budget):\n",
    "    \"\"\"Budget Constrained Softmax function for vector.\n",
    "    This function is more numerically stable than `_bcsoftmax1d` by computing some values in log-scale.\n",
    "    \n",
    "    Args:\n",
    "        x (Tensor): input vector. shape: (n_outputs, )\n",
    "        budget (Tensor): budget (constraint) vector. shape: (n_outputs, )\n",
    "\n",
    "    Returns:\n",
    "        y (Tensor): output probability vector. shape: (n_outputs, ). Satisfying the constraints y_i <= budget_i.\n",
    "    \n",
    "    \"\"\"\n",
    "    # sorting\n",
    "    _, indices = torch.sort(torch.log(budget) - x, descending=False)\n",
    "    x = x[indices]\n",
    "    budget = budget[indices]\n",
    "    # find K_B\n",
    "    log_r = torch.flip(torch.logcumsumexp(torch.flip(x, dims=(0, )), dim=0), dims=(0, ))\n",
    "    s = 1.0 - (torch.cumsum(budget, dim=0) - budget)\n",
    "    is_in_KB = torch.logical_or(\n",
    "        budget == 0,\n",
    "        torch.logical_and(\n",
    "            s - budget > 0,\n",
    "            x - log_r + torch.log(s) > torch.log(budget)\n",
    "        )\n",
    "    )\n",
    "    # compute outputs\n",
    "    exp_x = torch.exp(x - torch.max(torch.where(~is_in_KB, x, -torch.inf), dim=0)[0])\n",
    "    s = 1 - torch.sum(budget * is_in_KB)\n",
    "    r = torch.sum(exp_x * (~is_in_KB))\n",
    "    y = torch.where(~is_in_KB, s * exp_x / r, budget)\n",
    "    # undo sorting\n",
    "    _, inv_indices = torch.sort(indices, descending=False)\n",
    "    return y[inv_indices]\n",
    "\n",
    "\n",
    "class BCSoftmax1d(torch.autograd.Function):\n",
    "    \"\"\"Autograd implementation of Budget Constrained Softmax function for vector.\n",
    "    \"\"\"\n",
    "    generate_vmap_rule = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x, c):\n",
    "        y = _bcsoftmax1d_stable(x, c)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, c = inputs\n",
    "        is_in_KB = c == output\n",
    "        ctx.save_for_backward(x, c, is_in_KB)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y):\n",
    "        x, c, is_in_KB = ctx.saved_tensors\n",
    "        exp_x = torch.exp(\n",
    "            x - torch.max(torch.where(~is_in_KB, x, -torch.inf), dim=0)[0]\n",
    "        )\n",
    "        s = 1 - torch.sum(c * is_in_KB)\n",
    "        r = torch.sum(exp_x * (~is_in_KB))\n",
    "        \n",
    "        # compute Jacobian\n",
    "        Jx = torch.where(\n",
    "            torch.outer(~is_in_KB, ~is_in_KB),\n",
    "            torch.diag(~is_in_KB * exp_x) * r - torch.outer(exp_x, exp_x),\n",
    "            0,\n",
    "        )\n",
    "        Jx *= torch.where(\n",
    "            s > 0,\n",
    "            s / (r * r),\n",
    "            0\n",
    "        )\n",
    "        Jc = torch.where(\n",
    "            torch.outer(~is_in_KB, is_in_KB),\n",
    "            - exp_x[:, None] / r,\n",
    "            1.0 * torch.diag(is_in_KB)\n",
    "        )\n",
    "\n",
    "        # print(\"s\", s, \"r\", r)\n",
    "        # print(\"勾配\", torch.matmul(grad_y, Jx), torch.matmul(grad_y, Jc))\n",
    "        assert not torch.isnan(torch.matmul(grad_y, Jx)).any(), \"Jx contains NaN\"\n",
    "        assert not torch.isinf(torch.matmul(grad_y, Jx)).any(), \"Jx contains Inf\"\n",
    "        assert not torch.isnan(torch.matmul(grad_y, Jc)).any(), \"Jc contains NaN\"\n",
    "        assert not torch.isinf(torch.matmul(grad_y, Jc)).any(), \"Jc contains Inf\"\n",
    "\n",
    "        # return vector-Jacobian product\n",
    "        return torch.matmul(grad_y, Jx), torch.matmul(grad_y, Jc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Use these functions! #########\n",
    "bcsoftmax1d = BCSoftmax1d.apply\n",
    "\n",
    "# データによってmodelを通す回数が違\n",
    "# bcsoftmax2d = torch.vmap(BCSoftmax1d.apply) # input shape = (batch_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_questions):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(num_questions, num_questions, bias=False)  # 全結合層\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x = self.fc(x)  # 全結合層の適用\n",
    "        x = bcsoftmax1d(x, c)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"評価指標\"\n",
    "\n",
    "def kl_divergence(p, q, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    KLダイバージェンスを計算する関数（qが0の場合の無限大問題を回避）\n",
    "    \n",
    "    Parameters:\n",
    "        p (numpy.ndarray): 真の確率分布\n",
    "        q (numpy.ndarray): 予測確率分布\n",
    "        epsilon (float): スムージングパラメータ（非常に小さい値）\n",
    "    \n",
    "    Returns:\n",
    "        float: KLダイバージェンス\n",
    "    \"\"\"\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "    \n",
    "    # pとqにスムージングを適用\n",
    "    p = np.where(p == 0, epsilon, p)\n",
    "    q = np.where(q == 0, epsilon, q)\n",
    "    \n",
    "    # KLダイバージェンスの計算\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "def hellinger_distance(p, q):\n",
    "    \"\"\"\n",
    "    Hellinger距離を計算する関数\n",
    "\n",
    "    Parameters:\n",
    "        p (numpy.ndarray): 真の分布 [batch_size, num_classes]\n",
    "        q (numpy.ndarray): 予測分布 [batch_size, num_classes]\n",
    "    \n",
    "    Returns:\n",
    "        float: Hellinger距離の平均値\n",
    "    \"\"\"\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "    # Hellinger距離の計算式: H(p, q) = (1/√2) * ||√p - √q||_2\n",
    "    sqrt_p = np.sqrt(p)\n",
    "    sqrt_q = np.sqrt(q)\n",
    "    distance = np.sqrt(np.sum((sqrt_p - sqrt_q) ** 2)) / np.sqrt(2)\n",
    "    return np.mean(distance)\n",
    "\n",
    "def jensen_shannon_divergence(p, q):\n",
    "    \"\"\"\n",
    "    Jensen-Shannon divergence を計算する関数\n",
    "\n",
    "    Parameters:\n",
    "        p (numpy.ndarray): 真の分布 [batch_size, num_classes]\n",
    "        q (numpy.ndarray): 予測分布 [batch_size, num_classes]\n",
    "    \n",
    "    Returns:\n",
    "        float: Jensen-Shannon divergence の平均値\n",
    "    \"\"\"\n",
    "    p = np.array(p, dtype=np.float64)\n",
    "    q = np.array(q, dtype=np.float64)\n",
    "\n",
    "    # 0 の値があると log(0) が発生するため、微小値を加える\n",
    "    epsilon = 1e-12\n",
    "    p = np.clip(p, epsilon, 1)\n",
    "    q = np.clip(q, epsilon, 1)\n",
    "    \n",
    "    # 分布の平均 M\n",
    "    m = 0.5 * (p + q)\n",
    "    \n",
    "    # Kullback-Leibler divergence の補助関数\n",
    "    def kl(a, b):\n",
    "        return np.sum(a * np.log(a / b))\n",
    "    \n",
    "    # JSD の計算: JSD(p || q) = 0.5 * (KL(p || M) + KL(q || M))\n",
    "    jsd = 0.5 * (kl(p, m) + kl(q, m))\n",
    "    \n",
    "    # 各サンプルの平均を返す\n",
    "    return np.mean(jsd)\n",
    "\n",
    "\n",
    "def evaluate_model(r, p, q):\n",
    "    \"\"\"\n",
    "    遷移確率の真値と予測値の類似性を評価する関数\n",
    "    :param r: 各ノードの重み (np.array)\n",
    "    :param p: 各ノードの真の遷移確率 (2D np.array: ノード数 x 遷移確率)\n",
    "    :param q: 各ノードの予測遷移確率 (2D np.array: ノード数 x 遷移確率)\n",
    "    :return: 評価指標 L\n",
    "    \"\"\"\n",
    "    L = 0\n",
    "    for k in range(len(r)):\n",
    "        L += r[k] * kl_divergence(p[k], q[k])\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"実験結果を保存する外部ファイル\"\n",
    "import csv\n",
    "\n",
    "# 評価結果を保存するCSVファイル名\n",
    "results_csv = \"results_allpath_same_n_3000.csv\"\n",
    "\n",
    "# 初回にヘッダーを追加する\n",
    "header = [\n",
    "    \"Iteration\", \n",
    "    \"KL_Model_allpath_same_n\",\n",
    "    \"HD_Model_allpath_same_n\",\n",
    "    \"JSD_Model_allpath_same_n\",\n",
    "]\n",
    "\n",
    "try:\n",
    "    with open(results_csv, \"x\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "except FileExistsError:\n",
    "    pass  # ファイルが既に存在する場合は何もしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/773ptkr55z99zw26dvy19_v00000gn/T/ipykernel_68158/2263181784.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  train_X = torch.tensor(train_X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/3000], Loss: 1.5673\n",
      "Epoch [200/3000], Loss: 1.4157\n",
      "Epoch [300/3000], Loss: 1.3586\n",
      "Epoch [400/3000], Loss: 1.3346\n",
      "Epoch [500/3000], Loss: 1.3222\n",
      "Epoch [600/3000], Loss: 1.3150\n",
      "Epoch [700/3000], Loss: 1.3103\n",
      "Epoch [800/3000], Loss: 1.3071\n",
      "Epoch [900/3000], Loss: 1.3048\n",
      "Epoch [1000/3000], Loss: 1.3030\n",
      "Epoch [1100/3000], Loss: 1.3016\n",
      "Epoch [1200/3000], Loss: 1.3004\n",
      "Epoch [1300/3000], Loss: 1.2992\n",
      "Epoch [1400/3000], Loss: 1.2982\n",
      "Epoch [1500/3000], Loss: 1.2973\n",
      "Epoch [1600/3000], Loss: 1.2966\n",
      "Epoch [1700/3000], Loss: 1.2960\n",
      "Epoch [1800/3000], Loss: 1.2955\n",
      "Epoch [1900/3000], Loss: 1.2951\n",
      "Epoch [2000/3000], Loss: 1.2948\n",
      "Epoch [2100/3000], Loss: 1.2945\n",
      "Epoch [2200/3000], Loss: 1.2942\n",
      "Epoch [2300/3000], Loss: 1.2940\n",
      "Epoch [2400/3000], Loss: 1.2939\n",
      "Epoch [2500/3000], Loss: 1.2937\n",
      "Epoch [2600/3000], Loss: 1.2936\n",
      "Epoch [2700/3000], Loss: 1.2935\n",
      "Epoch [2800/3000], Loss: 1.2934\n",
      "Epoch [2900/3000], Loss: 1.2934\n",
      "Epoch [3000/3000], Loss: 1.2933\n",
      "Training complete!\n",
      "Trial 1 completed. Results saved.\n",
      "Trial 2\n",
      "Epoch [100/3000], Loss: 1.5309\n",
      "Epoch [200/3000], Loss: 1.3977\n",
      "Epoch [300/3000], Loss: 1.3496\n",
      "Epoch [400/3000], Loss: 1.3290\n",
      "Epoch [500/3000], Loss: 1.3182\n",
      "Epoch [600/3000], Loss: 1.3116\n",
      "Epoch [700/3000], Loss: 1.3070\n",
      "Epoch [800/3000], Loss: 1.3037\n",
      "Epoch [900/3000], Loss: 1.3013\n",
      "Epoch [1000/3000], Loss: 1.2994\n",
      "Epoch [1100/3000], Loss: 1.2979\n",
      "Epoch [1200/3000], Loss: 1.2966\n",
      "Epoch [1300/3000], Loss: 1.2955\n",
      "Epoch [1400/3000], Loss: 1.2944\n",
      "Epoch [1500/3000], Loss: 1.2934\n",
      "Epoch [1600/3000], Loss: 1.2926\n",
      "Epoch [1700/3000], Loss: 1.2920\n",
      "Epoch [1800/3000], Loss: 1.2914\n",
      "Epoch [1900/3000], Loss: 1.2910\n",
      "Epoch [2000/3000], Loss: 1.2907\n",
      "Epoch [2100/3000], Loss: 1.2904\n",
      "Epoch [2200/3000], Loss: 1.2902\n",
      "Epoch [2300/3000], Loss: 1.2900\n",
      "Epoch [2400/3000], Loss: 1.2899\n",
      "Epoch [2500/3000], Loss: 1.2898\n",
      "Epoch [2600/3000], Loss: 1.2897\n",
      "Epoch [2700/3000], Loss: 1.2896\n",
      "Epoch [2800/3000], Loss: 1.2895\n",
      "Epoch [2900/3000], Loss: 1.2895\n",
      "Epoch [3000/3000], Loss: 1.2894\n",
      "Training complete!\n",
      "Trial 2 completed. Results saved.\n",
      "Trial 3\n",
      "Epoch [100/3000], Loss: 1.5804\n",
      "Epoch [200/3000], Loss: 1.4179\n",
      "Epoch [300/3000], Loss: 1.3553\n",
      "Epoch [400/3000], Loss: 1.3289\n",
      "Epoch [500/3000], Loss: 1.3155\n",
      "Epoch [600/3000], Loss: 1.3074\n",
      "Epoch [700/3000], Loss: 1.3019\n",
      "Epoch [800/3000], Loss: 1.2982\n",
      "Epoch [900/3000], Loss: 1.2952\n",
      "Epoch [1000/3000], Loss: 1.2928\n",
      "Epoch [1100/3000], Loss: 1.2908\n",
      "Epoch [1200/3000], Loss: 1.2891\n",
      "Epoch [1300/3000], Loss: 1.2878\n",
      "Epoch [1400/3000], Loss: 1.2866\n",
      "Epoch [1500/3000], Loss: 1.2855\n",
      "Epoch [1600/3000], Loss: 1.2846\n",
      "Epoch [1700/3000], Loss: 1.2837\n",
      "Epoch [1800/3000], Loss: 1.2830\n",
      "Epoch [1900/3000], Loss: 1.2824\n",
      "Epoch [2000/3000], Loss: 1.2819\n",
      "Epoch [2100/3000], Loss: 1.2816\n",
      "Epoch [2200/3000], Loss: 1.2814\n",
      "Epoch [2300/3000], Loss: 1.2813\n",
      "Epoch [2400/3000], Loss: 1.2811\n",
      "Epoch [2500/3000], Loss: 1.2810\n",
      "Epoch [2600/3000], Loss: 1.2809\n",
      "Epoch [2700/3000], Loss: 1.2809\n",
      "Epoch [2800/3000], Loss: 1.2808\n",
      "Epoch [2900/3000], Loss: 1.2808\n",
      "Epoch [3000/3000], Loss: 1.2808\n",
      "Training complete!\n",
      "Trial 3 completed. Results saved.\n",
      "Trial 4\n",
      "Epoch [100/3000], Loss: 1.5471\n",
      "Epoch [200/3000], Loss: 1.4254\n",
      "Epoch [300/3000], Loss: 1.3749\n",
      "Epoch [400/3000], Loss: 1.3517\n",
      "Epoch [500/3000], Loss: 1.3399\n",
      "Epoch [600/3000], Loss: 1.3333\n",
      "Epoch [700/3000], Loss: 1.3293\n",
      "Epoch [800/3000], Loss: 1.3266\n",
      "Epoch [900/3000], Loss: 1.3248\n",
      "Epoch [1000/3000], Loss: 1.3235\n",
      "Epoch [1100/3000], Loss: 1.3224\n",
      "Epoch [1200/3000], Loss: 1.3216\n",
      "Epoch [1300/3000], Loss: 1.3208\n",
      "Epoch [1400/3000], Loss: 1.3201\n",
      "Epoch [1500/3000], Loss: 1.3195\n",
      "Epoch [1600/3000], Loss: 1.3190\n",
      "Epoch [1700/3000], Loss: 1.3185\n",
      "Epoch [1800/3000], Loss: 1.3180\n",
      "Epoch [1900/3000], Loss: 1.3175\n",
      "Epoch [2000/3000], Loss: 1.3171\n",
      "Epoch [2100/3000], Loss: 1.3167\n",
      "Epoch [2200/3000], Loss: 1.3165\n",
      "Epoch [2300/3000], Loss: 1.3164\n",
      "Epoch [2400/3000], Loss: 1.3164\n",
      "Epoch [2500/3000], Loss: 1.3163\n",
      "Epoch [2600/3000], Loss: 1.3163\n",
      "Epoch [2700/3000], Loss: 1.3163\n",
      "Epoch [2800/3000], Loss: 1.3163\n",
      "Epoch [2900/3000], Loss: 1.3163\n",
      "Epoch [3000/3000], Loss: 1.3163\n",
      "Training complete!\n",
      "Trial 4 completed. Results saved.\n",
      "Trial 5\n",
      "Epoch [100/3000], Loss: 1.5856\n",
      "Epoch [200/3000], Loss: 1.4640\n",
      "Epoch [300/3000], Loss: 1.4129\n",
      "Epoch [400/3000], Loss: 1.3891\n",
      "Epoch [500/3000], Loss: 1.3764\n",
      "Epoch [600/3000], Loss: 1.3687\n",
      "Epoch [700/3000], Loss: 1.3638\n",
      "Epoch [800/3000], Loss: 1.3606\n",
      "Epoch [900/3000], Loss: 1.3582\n",
      "Epoch [1000/3000], Loss: 1.3564\n",
      "Epoch [1100/3000], Loss: 1.3549\n",
      "Epoch [1200/3000], Loss: 1.3536\n",
      "Epoch [1300/3000], Loss: 1.3524\n",
      "Epoch [1400/3000], Loss: 1.3513\n",
      "Epoch [1500/3000], Loss: 1.3503\n",
      "Epoch [1600/3000], Loss: 1.3493\n",
      "Epoch [1700/3000], Loss: 1.3484\n",
      "Epoch [1800/3000], Loss: 1.3475\n",
      "Epoch [1900/3000], Loss: 1.3469\n",
      "Epoch [2000/3000], Loss: 1.3464\n",
      "Epoch [2100/3000], Loss: 1.3460\n",
      "Epoch [2200/3000], Loss: 1.3457\n",
      "Epoch [2300/3000], Loss: 1.3454\n",
      "Epoch [2400/3000], Loss: 1.3453\n",
      "Epoch [2500/3000], Loss: 1.3451\n",
      "Epoch [2600/3000], Loss: 1.3451\n",
      "Epoch [2700/3000], Loss: 1.3450\n",
      "Epoch [2800/3000], Loss: 1.3449\n",
      "Epoch [2900/3000], Loss: 1.3449\n",
      "Epoch [3000/3000], Loss: 1.3448\n",
      "Training complete!\n",
      "Trial 5 completed. Results saved.\n",
      "Trial 6\n",
      "Epoch [100/3000], Loss: 1.5538\n",
      "Epoch [200/3000], Loss: 1.4295\n",
      "Epoch [300/3000], Loss: 1.3800\n",
      "Epoch [400/3000], Loss: 1.3552\n",
      "Epoch [500/3000], Loss: 1.3405\n",
      "Epoch [600/3000], Loss: 1.3326\n",
      "Epoch [700/3000], Loss: 1.3277\n",
      "Epoch [800/3000], Loss: 1.3245\n",
      "Epoch [900/3000], Loss: 1.3221\n",
      "Epoch [1000/3000], Loss: 1.3202\n",
      "Epoch [1100/3000], Loss: 1.3186\n",
      "Epoch [1200/3000], Loss: 1.3175\n",
      "Epoch [1300/3000], Loss: 1.3167\n",
      "Epoch [1400/3000], Loss: 1.3161\n",
      "Epoch [1500/3000], Loss: 1.3157\n",
      "Epoch [1600/3000], Loss: 1.3153\n",
      "Epoch [1700/3000], Loss: 1.3150\n",
      "Epoch [1800/3000], Loss: 1.3148\n",
      "Epoch [1900/3000], Loss: 1.3146\n",
      "Epoch [2000/3000], Loss: 1.3144\n",
      "Epoch [2100/3000], Loss: 1.3143\n",
      "Epoch [2200/3000], Loss: 1.3142\n",
      "Epoch [2300/3000], Loss: 1.3141\n",
      "Epoch [2400/3000], Loss: 1.3140\n",
      "Epoch [2500/3000], Loss: 1.3140\n",
      "Epoch [2600/3000], Loss: 1.3139\n",
      "Epoch [2700/3000], Loss: 1.3138\n",
      "Epoch [2800/3000], Loss: 1.3138\n",
      "Epoch [2900/3000], Loss: 1.3137\n",
      "Epoch [3000/3000], Loss: 1.3137\n",
      "Training complete!\n",
      "Trial 6 completed. Results saved.\n",
      "Trial 7\n",
      "Epoch [100/3000], Loss: 1.5353\n",
      "Epoch [200/3000], Loss: 1.3977\n",
      "Epoch [300/3000], Loss: 1.3425\n",
      "Epoch [400/3000], Loss: 1.3162\n",
      "Epoch [500/3000], Loss: 1.3018\n",
      "Epoch [600/3000], Loss: 1.2933\n",
      "Epoch [700/3000], Loss: 1.2877\n",
      "Epoch [800/3000], Loss: 1.2837\n",
      "Epoch [900/3000], Loss: 1.2808\n",
      "Epoch [1000/3000], Loss: 1.2786\n",
      "Epoch [1100/3000], Loss: 1.2770\n",
      "Epoch [1200/3000], Loss: 1.2758\n",
      "Epoch [1300/3000], Loss: 1.2748\n",
      "Epoch [1400/3000], Loss: 1.2739\n",
      "Epoch [1500/3000], Loss: 1.2732\n",
      "Epoch [1600/3000], Loss: 1.2726\n",
      "Epoch [1700/3000], Loss: 1.2721\n",
      "Epoch [1800/3000], Loss: 1.2716\n",
      "Epoch [1900/3000], Loss: 1.2713\n",
      "Epoch [2000/3000], Loss: 1.2709\n",
      "Epoch [2100/3000], Loss: 1.2707\n",
      "Epoch [2200/3000], Loss: 1.2705\n",
      "Epoch [2300/3000], Loss: 1.2703\n",
      "Epoch [2400/3000], Loss: 1.2701\n",
      "Epoch [2500/3000], Loss: 1.2699\n",
      "Epoch [2600/3000], Loss: 1.2698\n",
      "Epoch [2700/3000], Loss: 1.2697\n",
      "Epoch [2800/3000], Loss: 1.2696\n",
      "Epoch [2900/3000], Loss: 1.2695\n",
      "Epoch [3000/3000], Loss: 1.2694\n",
      "Training complete!\n",
      "Trial 7 completed. Results saved.\n",
      "Trial 8\n",
      "Epoch [100/3000], Loss: 1.5887\n",
      "Epoch [200/3000], Loss: 1.4606\n",
      "Epoch [300/3000], Loss: 1.4117\n",
      "Epoch [400/3000], Loss: 1.3891\n",
      "Epoch [500/3000], Loss: 1.3771\n",
      "Epoch [600/3000], Loss: 1.3701\n",
      "Epoch [700/3000], Loss: 1.3656\n",
      "Epoch [800/3000], Loss: 1.3622\n",
      "Epoch [900/3000], Loss: 1.3597\n",
      "Epoch [1000/3000], Loss: 1.3576\n",
      "Epoch [1100/3000], Loss: 1.3558\n",
      "Epoch [1200/3000], Loss: 1.3543\n",
      "Epoch [1300/3000], Loss: 1.3528\n",
      "Epoch [1400/3000], Loss: 1.3515\n",
      "Epoch [1500/3000], Loss: 1.3502\n",
      "Epoch [1600/3000], Loss: 1.3494\n",
      "Epoch [1700/3000], Loss: 1.3490\n",
      "Epoch [1800/3000], Loss: 1.3487\n",
      "Epoch [1900/3000], Loss: 1.3485\n",
      "Epoch [2000/3000], Loss: 1.3483\n",
      "Epoch [2100/3000], Loss: 1.3481\n",
      "Epoch [2200/3000], Loss: 1.3480\n",
      "Epoch [2300/3000], Loss: 1.3479\n",
      "Epoch [2400/3000], Loss: 1.3478\n",
      "Epoch [2500/3000], Loss: 1.3477\n",
      "Epoch [2600/3000], Loss: 1.3477\n",
      "Epoch [2700/3000], Loss: 1.3476\n",
      "Epoch [2800/3000], Loss: 1.3476\n",
      "Epoch [2900/3000], Loss: 1.3476\n",
      "Epoch [3000/3000], Loss: 1.3476\n",
      "Training complete!\n",
      "Trial 8 completed. Results saved.\n",
      "Trial 9\n",
      "Epoch [100/3000], Loss: 1.5356\n",
      "Epoch [200/3000], Loss: 1.3873\n",
      "Epoch [300/3000], Loss: 1.3296\n",
      "Epoch [400/3000], Loss: 1.3041\n",
      "Epoch [500/3000], Loss: 1.2905\n",
      "Epoch [600/3000], Loss: 1.2827\n",
      "Epoch [700/3000], Loss: 1.2775\n",
      "Epoch [800/3000], Loss: 1.2738\n",
      "Epoch [900/3000], Loss: 1.2708\n",
      "Epoch [1000/3000], Loss: 1.2684\n",
      "Epoch [1100/3000], Loss: 1.2665\n",
      "Epoch [1200/3000], Loss: 1.2649\n",
      "Epoch [1300/3000], Loss: 1.2635\n",
      "Epoch [1400/3000], Loss: 1.2623\n",
      "Epoch [1500/3000], Loss: 1.2613\n",
      "Epoch [1600/3000], Loss: 1.2605\n",
      "Epoch [1700/3000], Loss: 1.2600\n",
      "Epoch [1800/3000], Loss: 1.2596\n",
      "Epoch [1900/3000], Loss: 1.2593\n",
      "Epoch [2000/3000], Loss: 1.2591\n",
      "Epoch [2100/3000], Loss: 1.2589\n",
      "Epoch [2200/3000], Loss: 1.2588\n",
      "Epoch [2300/3000], Loss: 1.2588\n",
      "Epoch [2400/3000], Loss: 1.2587\n",
      "Epoch [2500/3000], Loss: 1.2587\n",
      "Epoch [2600/3000], Loss: 1.2587\n",
      "Epoch [2700/3000], Loss: 1.2586\n",
      "Epoch [2800/3000], Loss: 1.2586\n",
      "Epoch [2900/3000], Loss: 1.2586\n",
      "Epoch [3000/3000], Loss: 1.2586\n",
      "Training complete!\n",
      "Trial 9 completed. Results saved.\n",
      "Trial 10\n",
      "Epoch [100/3000], Loss: 1.5561\n",
      "Epoch [200/3000], Loss: 1.4108\n",
      "Epoch [300/3000], Loss: 1.3547\n",
      "Epoch [400/3000], Loss: 1.3305\n",
      "Epoch [500/3000], Loss: 1.3163\n",
      "Epoch [600/3000], Loss: 1.3065\n",
      "Epoch [700/3000], Loss: 1.3005\n",
      "Epoch [800/3000], Loss: 1.2964\n",
      "Epoch [900/3000], Loss: 1.2934\n",
      "Epoch [1000/3000], Loss: 1.2913\n",
      "Epoch [1100/3000], Loss: 1.2898\n",
      "Epoch [1200/3000], Loss: 1.2886\n",
      "Epoch [1300/3000], Loss: 1.2877\n",
      "Epoch [1400/3000], Loss: 1.2869\n",
      "Epoch [1500/3000], Loss: 1.2863\n",
      "Epoch [1600/3000], Loss: 1.2858\n",
      "Epoch [1700/3000], Loss: 1.2854\n",
      "Epoch [1800/3000], Loss: 1.2851\n",
      "Epoch [1900/3000], Loss: 1.2848\n",
      "Epoch [2000/3000], Loss: 1.2846\n",
      "Epoch [2100/3000], Loss: 1.2844\n",
      "Epoch [2200/3000], Loss: 1.2843\n",
      "Epoch [2300/3000], Loss: 1.2841\n",
      "Epoch [2400/3000], Loss: 1.2840\n",
      "Epoch [2500/3000], Loss: 1.2839\n",
      "Epoch [2600/3000], Loss: 1.2838\n",
      "Epoch [2700/3000], Loss: 1.2837\n",
      "Epoch [2800/3000], Loss: 1.2836\n",
      "Epoch [2900/3000], Loss: 1.2836\n",
      "Epoch [3000/3000], Loss: 1.2835\n",
      "Training complete!\n",
      "Trial 10 completed. Results saved.\n",
      "Trial 11\n",
      "Epoch [100/3000], Loss: 1.4886\n",
      "Epoch [200/3000], Loss: 1.3312\n",
      "Epoch [300/3000], Loss: 1.2733\n",
      "Epoch [400/3000], Loss: 1.2481\n",
      "Epoch [500/3000], Loss: 1.2347\n",
      "Epoch [600/3000], Loss: 1.2266\n",
      "Epoch [700/3000], Loss: 1.2210\n",
      "Epoch [800/3000], Loss: 1.2168\n",
      "Epoch [900/3000], Loss: 1.2136\n",
      "Epoch [1000/3000], Loss: 1.2111\n",
      "Epoch [1100/3000], Loss: 1.2091\n",
      "Epoch [1200/3000], Loss: 1.2075\n",
      "Epoch [1300/3000], Loss: 1.2063\n",
      "Epoch [1400/3000], Loss: 1.2052\n",
      "Epoch [1500/3000], Loss: 1.2043\n",
      "Epoch [1600/3000], Loss: 1.2035\n",
      "Epoch [1700/3000], Loss: 1.2027\n",
      "Epoch [1800/3000], Loss: 1.2020\n",
      "Epoch [1900/3000], Loss: 1.2014\n",
      "Epoch [2000/3000], Loss: 1.2008\n",
      "Epoch [2100/3000], Loss: 1.2003\n",
      "Epoch [2200/3000], Loss: 1.1998\n",
      "Epoch [2300/3000], Loss: 1.1994\n",
      "Epoch [2400/3000], Loss: 1.1991\n",
      "Epoch [2500/3000], Loss: 1.1988\n",
      "Epoch [2600/3000], Loss: 1.1986\n",
      "Epoch [2700/3000], Loss: 1.1985\n",
      "Epoch [2800/3000], Loss: 1.1983\n",
      "Epoch [2900/3000], Loss: 1.1982\n",
      "Epoch [3000/3000], Loss: 1.1982\n",
      "Training complete!\n",
      "Trial 11 completed. Results saved.\n",
      "Trial 12\n",
      "Epoch [100/3000], Loss: 1.5688\n",
      "Epoch [200/3000], Loss: 1.4263\n",
      "Epoch [300/3000], Loss: 1.3626\n",
      "Epoch [400/3000], Loss: 1.3353\n",
      "Epoch [500/3000], Loss: 1.3220\n",
      "Epoch [600/3000], Loss: 1.3144\n",
      "Epoch [700/3000], Loss: 1.3095\n",
      "Epoch [800/3000], Loss: 1.3060\n",
      "Epoch [900/3000], Loss: 1.3034\n",
      "Epoch [1000/3000], Loss: 1.3017\n",
      "Epoch [1100/3000], Loss: 1.3004\n",
      "Epoch [1200/3000], Loss: 1.2994\n",
      "Epoch [1300/3000], Loss: 1.2986\n",
      "Epoch [1400/3000], Loss: 1.2978\n",
      "Epoch [1500/3000], Loss: 1.2971\n",
      "Epoch [1600/3000], Loss: 1.2966\n",
      "Epoch [1700/3000], Loss: 1.2962\n",
      "Epoch [1800/3000], Loss: 1.2958\n",
      "Epoch [1900/3000], Loss: 1.2956\n",
      "Epoch [2000/3000], Loss: 1.2953\n",
      "Epoch [2100/3000], Loss: 1.2951\n",
      "Epoch [2200/3000], Loss: 1.2950\n",
      "Epoch [2300/3000], Loss: 1.2948\n",
      "Epoch [2400/3000], Loss: 1.2948\n",
      "Epoch [2500/3000], Loss: 1.2947\n",
      "Epoch [2600/3000], Loss: 1.2946\n",
      "Epoch [2700/3000], Loss: 1.2946\n",
      "Epoch [2800/3000], Loss: 1.2945\n",
      "Epoch [2900/3000], Loss: 1.2945\n",
      "Epoch [3000/3000], Loss: 1.2945\n",
      "Training complete!\n",
      "Trial 12 completed. Results saved.\n",
      "Trial 13\n",
      "Epoch [100/3000], Loss: 1.5197\n",
      "Epoch [200/3000], Loss: 1.3838\n",
      "Epoch [300/3000], Loss: 1.3296\n",
      "Epoch [400/3000], Loss: 1.3053\n",
      "Epoch [500/3000], Loss: 1.2928\n",
      "Epoch [600/3000], Loss: 1.2856\n",
      "Epoch [700/3000], Loss: 1.2808\n",
      "Epoch [800/3000], Loss: 1.2775\n",
      "Epoch [900/3000], Loss: 1.2749\n",
      "Epoch [1000/3000], Loss: 1.2728\n",
      "Epoch [1100/3000], Loss: 1.2710\n",
      "Epoch [1200/3000], Loss: 1.2693\n",
      "Epoch [1300/3000], Loss: 1.2680\n",
      "Epoch [1400/3000], Loss: 1.2669\n",
      "Epoch [1500/3000], Loss: 1.2658\n",
      "Epoch [1600/3000], Loss: 1.2650\n",
      "Epoch [1700/3000], Loss: 1.2642\n",
      "Epoch [1800/3000], Loss: 1.2634\n",
      "Epoch [1900/3000], Loss: 1.2629\n",
      "Epoch [2000/3000], Loss: 1.2625\n",
      "Epoch [2100/3000], Loss: 1.2622\n",
      "Epoch [2200/3000], Loss: 1.2619\n",
      "Epoch [2300/3000], Loss: 1.2618\n",
      "Epoch [2400/3000], Loss: 1.2617\n",
      "Epoch [2500/3000], Loss: 1.2616\n",
      "Epoch [2600/3000], Loss: 1.2615\n",
      "Epoch [2700/3000], Loss: 1.2615\n",
      "Epoch [2800/3000], Loss: 1.2615\n",
      "Epoch [2900/3000], Loss: 1.2615\n",
      "Epoch [3000/3000], Loss: 1.2615\n",
      "Training complete!\n",
      "Trial 13 completed. Results saved.\n",
      "Trial 14\n",
      "Epoch [100/3000], Loss: 1.5532\n",
      "Epoch [200/3000], Loss: 1.4036\n",
      "Epoch [300/3000], Loss: 1.3459\n",
      "Epoch [400/3000], Loss: 1.3213\n",
      "Epoch [500/3000], Loss: 1.3085\n",
      "Epoch [600/3000], Loss: 1.3009\n",
      "Epoch [700/3000], Loss: 1.2958\n",
      "Epoch [800/3000], Loss: 1.2924\n",
      "Epoch [900/3000], Loss: 1.2899\n",
      "Epoch [1000/3000], Loss: 1.2878\n",
      "Epoch [1100/3000], Loss: 1.2861\n",
      "Epoch [1200/3000], Loss: 1.2846\n",
      "Epoch [1300/3000], Loss: 1.2832\n",
      "Epoch [1400/3000], Loss: 1.2820\n",
      "Epoch [1500/3000], Loss: 1.2809\n",
      "Epoch [1600/3000], Loss: 1.2799\n",
      "Epoch [1700/3000], Loss: 1.2789\n",
      "Epoch [1800/3000], Loss: 1.2781\n",
      "Epoch [1900/3000], Loss: 1.2773\n",
      "Epoch [2000/3000], Loss: 1.2767\n",
      "Epoch [2100/3000], Loss: 1.2761\n",
      "Epoch [2200/3000], Loss: 1.2756\n",
      "Epoch [2300/3000], Loss: 1.2753\n",
      "Epoch [2400/3000], Loss: 1.2750\n",
      "Epoch [2500/3000], Loss: 1.2748\n",
      "Epoch [2600/3000], Loss: 1.2746\n",
      "Epoch [2700/3000], Loss: 1.2745\n",
      "Epoch [2800/3000], Loss: 1.2743\n",
      "Epoch [2900/3000], Loss: 1.2743\n",
      "Epoch [3000/3000], Loss: 1.2742\n",
      "Training complete!\n",
      "Trial 14 completed. Results saved.\n",
      "Trial 15\n",
      "Epoch [100/3000], Loss: 1.5555\n",
      "Epoch [200/3000], Loss: 1.3990\n",
      "Epoch [300/3000], Loss: 1.3424\n",
      "Epoch [400/3000], Loss: 1.3175\n",
      "Epoch [500/3000], Loss: 1.3046\n",
      "Epoch [600/3000], Loss: 1.2968\n",
      "Epoch [700/3000], Loss: 1.2915\n",
      "Epoch [800/3000], Loss: 1.2877\n",
      "Epoch [900/3000], Loss: 1.2848\n",
      "Epoch [1000/3000], Loss: 1.2825\n",
      "Epoch [1100/3000], Loss: 1.2809\n",
      "Epoch [1200/3000], Loss: 1.2795\n",
      "Epoch [1300/3000], Loss: 1.2783\n",
      "Epoch [1400/3000], Loss: 1.2774\n",
      "Epoch [1500/3000], Loss: 1.2766\n",
      "Epoch [1600/3000], Loss: 1.2759\n",
      "Epoch [1700/3000], Loss: 1.2753\n",
      "Epoch [1800/3000], Loss: 1.2748\n",
      "Epoch [1900/3000], Loss: 1.2744\n",
      "Epoch [2000/3000], Loss: 1.2740\n",
      "Epoch [2100/3000], Loss: 1.2737\n",
      "Epoch [2200/3000], Loss: 1.2735\n",
      "Epoch [2300/3000], Loss: 1.2734\n",
      "Epoch [2400/3000], Loss: 1.2732\n",
      "Epoch [2500/3000], Loss: 1.2731\n",
      "Epoch [2600/3000], Loss: 1.2731\n",
      "Epoch [2700/3000], Loss: 1.2730\n",
      "Epoch [2800/3000], Loss: 1.2730\n",
      "Epoch [2900/3000], Loss: 1.2730\n",
      "Epoch [3000/3000], Loss: 1.2729\n",
      "Training complete!\n",
      "Trial 15 completed. Results saved.\n",
      "Trial 16\n",
      "Epoch [100/3000], Loss: 1.5638\n",
      "Epoch [200/3000], Loss: 1.4293\n",
      "Epoch [300/3000], Loss: 1.3790\n",
      "Epoch [400/3000], Loss: 1.3571\n",
      "Epoch [500/3000], Loss: 1.3450\n",
      "Epoch [600/3000], Loss: 1.3381\n",
      "Epoch [700/3000], Loss: 1.3338\n",
      "Epoch [800/3000], Loss: 1.3308\n",
      "Epoch [900/3000], Loss: 1.3285\n",
      "Epoch [1000/3000], Loss: 1.3267\n",
      "Epoch [1100/3000], Loss: 1.3250\n",
      "Epoch [1200/3000], Loss: 1.3234\n",
      "Epoch [1300/3000], Loss: 1.3218\n",
      "Epoch [1400/3000], Loss: 1.3205\n",
      "Epoch [1500/3000], Loss: 1.3195\n",
      "Epoch [1600/3000], Loss: 1.3186\n",
      "Epoch [1700/3000], Loss: 1.3179\n",
      "Epoch [1800/3000], Loss: 1.3173\n",
      "Epoch [1900/3000], Loss: 1.3169\n",
      "Epoch [2000/3000], Loss: 1.3166\n",
      "Epoch [2100/3000], Loss: 1.3164\n",
      "Epoch [2200/3000], Loss: 1.3162\n",
      "Epoch [2300/3000], Loss: 1.3161\n",
      "Epoch [2400/3000], Loss: 1.3159\n",
      "Epoch [2500/3000], Loss: 1.3158\n",
      "Epoch [2600/3000], Loss: 1.3158\n",
      "Epoch [2700/3000], Loss: 1.3157\n",
      "Epoch [2800/3000], Loss: 1.3157\n",
      "Epoch [2900/3000], Loss: 1.3157\n",
      "Epoch [3000/3000], Loss: 1.3156\n",
      "Training complete!\n",
      "Trial 16 completed. Results saved.\n",
      "Trial 17\n",
      "Epoch [100/3000], Loss: 1.5218\n",
      "Epoch [200/3000], Loss: 1.3977\n",
      "Epoch [300/3000], Loss: 1.3514\n",
      "Epoch [400/3000], Loss: 1.3303\n",
      "Epoch [500/3000], Loss: 1.3191\n",
      "Epoch [600/3000], Loss: 1.3123\n",
      "Epoch [700/3000], Loss: 1.3077\n",
      "Epoch [800/3000], Loss: 1.3043\n",
      "Epoch [900/3000], Loss: 1.3017\n",
      "Epoch [1000/3000], Loss: 1.2997\n",
      "Epoch [1100/3000], Loss: 1.2980\n",
      "Epoch [1200/3000], Loss: 1.2966\n",
      "Epoch [1300/3000], Loss: 1.2954\n",
      "Epoch [1400/3000], Loss: 1.2944\n",
      "Epoch [1500/3000], Loss: 1.2935\n",
      "Epoch [1600/3000], Loss: 1.2927\n",
      "Epoch [1700/3000], Loss: 1.2921\n",
      "Epoch [1800/3000], Loss: 1.2916\n",
      "Epoch [1900/3000], Loss: 1.2913\n",
      "Epoch [2000/3000], Loss: 1.2911\n",
      "Epoch [2100/3000], Loss: 1.2909\n",
      "Epoch [2200/3000], Loss: 1.2908\n",
      "Epoch [2300/3000], Loss: 1.2907\n",
      "Epoch [2400/3000], Loss: 1.2907\n",
      "Epoch [2500/3000], Loss: 1.2906\n",
      "Epoch [2600/3000], Loss: 1.2906\n",
      "Epoch [2700/3000], Loss: 1.2906\n",
      "Epoch [2800/3000], Loss: 1.2905\n",
      "Epoch [2900/3000], Loss: 1.2905\n",
      "Epoch [3000/3000], Loss: 1.2905\n",
      "Training complete!\n",
      "Trial 17 completed. Results saved.\n",
      "Trial 18\n",
      "Epoch [100/3000], Loss: 1.5420\n",
      "Epoch [200/3000], Loss: 1.3921\n",
      "Epoch [300/3000], Loss: 1.3364\n",
      "Epoch [400/3000], Loss: 1.3128\n",
      "Epoch [500/3000], Loss: 1.3009\n",
      "Epoch [600/3000], Loss: 1.2940\n",
      "Epoch [700/3000], Loss: 1.2894\n",
      "Epoch [800/3000], Loss: 1.2861\n",
      "Epoch [900/3000], Loss: 1.2835\n",
      "Epoch [1000/3000], Loss: 1.2813\n",
      "Epoch [1100/3000], Loss: 1.2796\n",
      "Epoch [1200/3000], Loss: 1.2783\n",
      "Epoch [1300/3000], Loss: 1.2774\n",
      "Epoch [1400/3000], Loss: 1.2766\n",
      "Epoch [1500/3000], Loss: 1.2759\n",
      "Epoch [1600/3000], Loss: 1.2753\n",
      "Epoch [1700/3000], Loss: 1.2748\n",
      "Epoch [1800/3000], Loss: 1.2745\n",
      "Epoch [1900/3000], Loss: 1.2743\n",
      "Epoch [2000/3000], Loss: 1.2742\n",
      "Epoch [2100/3000], Loss: 1.2741\n",
      "Epoch [2200/3000], Loss: 1.2740\n",
      "Epoch [2300/3000], Loss: 1.2740\n",
      "Epoch [2400/3000], Loss: 1.2739\n",
      "Epoch [2500/3000], Loss: 1.2739\n",
      "Epoch [2600/3000], Loss: 1.2739\n",
      "Epoch [2700/3000], Loss: 1.2738\n",
      "Epoch [2800/3000], Loss: 1.2738\n",
      "Epoch [2900/3000], Loss: 1.2738\n",
      "Epoch [3000/3000], Loss: 1.2738\n",
      "Training complete!\n",
      "Trial 18 completed. Results saved.\n",
      "Trial 19\n",
      "Epoch [100/3000], Loss: 1.5584\n",
      "Epoch [200/3000], Loss: 1.4078\n",
      "Epoch [300/3000], Loss: 1.3540\n",
      "Epoch [400/3000], Loss: 1.3301\n",
      "Epoch [500/3000], Loss: 1.3176\n",
      "Epoch [600/3000], Loss: 1.3101\n",
      "Epoch [700/3000], Loss: 1.3051\n",
      "Epoch [800/3000], Loss: 1.3017\n",
      "Epoch [900/3000], Loss: 1.2992\n",
      "Epoch [1000/3000], Loss: 1.2973\n",
      "Epoch [1100/3000], Loss: 1.2959\n",
      "Epoch [1200/3000], Loss: 1.2947\n",
      "Epoch [1300/3000], Loss: 1.2938\n",
      "Epoch [1400/3000], Loss: 1.2930\n",
      "Epoch [1500/3000], Loss: 1.2922\n",
      "Epoch [1600/3000], Loss: 1.2915\n",
      "Epoch [1700/3000], Loss: 1.2908\n",
      "Epoch [1800/3000], Loss: 1.2903\n",
      "Epoch [1900/3000], Loss: 1.2898\n",
      "Epoch [2000/3000], Loss: 1.2893\n",
      "Epoch [2100/3000], Loss: 1.2890\n",
      "Epoch [2200/3000], Loss: 1.2887\n",
      "Epoch [2300/3000], Loss: 1.2885\n",
      "Epoch [2400/3000], Loss: 1.2884\n",
      "Epoch [2500/3000], Loss: 1.2883\n",
      "Epoch [2600/3000], Loss: 1.2882\n",
      "Epoch [2700/3000], Loss: 1.2881\n",
      "Epoch [2800/3000], Loss: 1.2881\n",
      "Epoch [2900/3000], Loss: 1.2880\n",
      "Epoch [3000/3000], Loss: 1.2880\n",
      "Training complete!\n",
      "Trial 19 completed. Results saved.\n",
      "Trial 20\n",
      "Epoch [100/3000], Loss: 1.5825\n",
      "Epoch [200/3000], Loss: 1.4239\n",
      "Epoch [300/3000], Loss: 1.3575\n",
      "Epoch [400/3000], Loss: 1.3313\n",
      "Epoch [500/3000], Loss: 1.3172\n",
      "Epoch [600/3000], Loss: 1.3088\n",
      "Epoch [700/3000], Loss: 1.3038\n",
      "Epoch [800/3000], Loss: 1.3003\n",
      "Epoch [900/3000], Loss: 1.2977\n",
      "Epoch [1000/3000], Loss: 1.2955\n",
      "Epoch [1100/3000], Loss: 1.2936\n",
      "Epoch [1200/3000], Loss: 1.2919\n",
      "Epoch [1300/3000], Loss: 1.2904\n",
      "Epoch [1400/3000], Loss: 1.2890\n",
      "Epoch [1500/3000], Loss: 1.2878\n",
      "Epoch [1600/3000], Loss: 1.2869\n",
      "Epoch [1700/3000], Loss: 1.2862\n",
      "Epoch [1800/3000], Loss: 1.2856\n",
      "Epoch [1900/3000], Loss: 1.2852\n",
      "Epoch [2000/3000], Loss: 1.2848\n",
      "Epoch [2100/3000], Loss: 1.2844\n",
      "Epoch [2200/3000], Loss: 1.2841\n",
      "Epoch [2300/3000], Loss: 1.2838\n",
      "Epoch [2400/3000], Loss: 1.2836\n",
      "Epoch [2500/3000], Loss: 1.2833\n",
      "Epoch [2600/3000], Loss: 1.2831\n",
      "Epoch [2700/3000], Loss: 1.2829\n",
      "Epoch [2800/3000], Loss: 1.2827\n",
      "Epoch [2900/3000], Loss: 1.2826\n",
      "Epoch [3000/3000], Loss: 1.2824\n",
      "Training complete!\n",
      "Trial 20 completed. Results saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# データセットの生成\n",
    "num_questions = 5  # 問題数\n",
    "num_data_per_step = 20     # 各ステップごとに生成するデータ数\n",
    "\n",
    "num_epochs = 3000  # エポック数\n",
    "alpha = 0.001 # 正則化パラメータ\n",
    "\n",
    "for trial in range(20):\n",
    "    print(f\"Trial {trial + 1}\")\n",
    "\n",
    "    \"データ生成\"\n",
    "\n",
    "    # 生徒の回答状況 X (1が正解、0が不正解)\n",
    "    # 初期状態は全て不正解\n",
    "    X_init = np.array([1, 0, 0, 0, 0, 0])\n",
    "\n",
    "    training_data = generate_training_data(A, X_init, num_questions, num_data_per_step)\n",
    "    train_X = [input_data for input_data, _ in training_data]\n",
    "    train_Y = [target_data for _, target_data in training_data]\n",
    "\n",
    "    # PyTorch テンソルに変換\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float32)\n",
    "    train_Y = torch.tensor(train_Y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    \"状態定義\"\n",
    "\n",
    "    # Generate states where the first digit is always 1\n",
    "    states = [(1,) + state for state in itertools.product([0, 1], repeat=num_questions)]\n",
    "\n",
    "    states = sorted(states, key=lambda state: sum(state))\n",
    "\n",
    "    # Initialize the state counts\n",
    "    state_counts = defaultdict(int)\n",
    "\n",
    "    # Assuming 'dataset' is your list of student results\n",
    "    for result, result2 in training_data:\n",
    "        # print(result2)\n",
    "        state_tuple = tuple(map(int, result + result2))  # Convert np.int64 to int\n",
    "        state_counts[state_tuple] += 1  # Count only if the first digit is 1\n",
    "\n",
    "    # Display the counts for each state\n",
    "    for state in states:\n",
    "        count = state_counts[state]\n",
    "        formatted_state = list(state)  # Convert tuple to list for the desired format\n",
    "\n",
    "    \n",
    "    \"モデル定義\"\n",
    "    # モデル、損失関数、最適化関数の設定\n",
    "    model = Model(num_questions+1)  # 5問+初期状態の問題を扱うモデル\n",
    "    criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "    \"学習\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # モデルを訓練モードに\n",
    "        optimizer.zero_grad()  # 勾配の初期化\n",
    "        \n",
    "        outputs = []  # 出力を保持するリスト\n",
    "        # 各データに対して train_Y の値に基づいてループを実行\n",
    "        for i, target in enumerate(train_Y):\n",
    "            c = torch.ones(num_questions+1, dtype=torch.float32)\n",
    "            output = train_X[i]  # 各 i 番目の入力データを使用\n",
    "            output = output.view(-1)\n",
    "\n",
    "            # もしcの和が1なら、rが0となるので対応\n",
    "            if c.sum() <= 1:\n",
    "                # print(\"aaaaaaaa\")\n",
    "                output = c\n",
    "            else:\n",
    "                output = model(output, c)  # 前回の出力を次のステップの入力として使用\n",
    "\n",
    "            outputs.append(output)  # 最終的な出力を保存\n",
    "\n",
    "        # outputs を適切な形に変換して損失計算（例えば torch.stack を使用）\n",
    "        outputs = torch.stack(outputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        # モデルの出力の確認\n",
    "        assert not torch.isnan(outputs).any(), \"Model output contains NaN\"\n",
    "        assert not torch.isinf(outputs).any(), \"Model output contains Inf\"\n",
    "\n",
    "        # L1正則化項の計算\n",
    "        l1_reg = torch.tensor(0.0, requires_grad=True)\n",
    "        for param in model.parameters():\n",
    "            l1_reg = l1_reg + torch.sum(torch.abs(param))\n",
    "\n",
    "        # 損失の計算\n",
    "        loss0 = criterion(outputs, train_Y)\n",
    "\n",
    "        loss = loss0 + alpha * l1_reg  # L1正則化項を追加した損失\n",
    "        \n",
    "        # 損失の確認\n",
    "        assert not torch.isnan(loss).any(), \"Loss contains NaN\"\n",
    "        assert not torch.isinf(loss).any(), \"Loss contains Inf\"\n",
    "        \n",
    "        # バックプロパゲーションとパラメータの更新\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 100エポックごとに損失を表示\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # 学習結果の確認\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    \"モデル評価\"\n",
    "    node_probabilities = defaultdict(float)\n",
    "    start = (1,) + tuple(0 for _ in range(num_questions))\n",
    "    node_probabilities[start] = 1\n",
    "\n",
    "    # 機械学習モデルと簡易モデル\n",
    "    KL = 0\n",
    "    HD = 0\n",
    "    JSD = 0\n",
    "\n",
    "    for state in states:\n",
    "        probabilities = calculate_transition_probabilities(A, np.array(state))\n",
    "\n",
    "        # 最下層から頂点までのノードの分布を順に計算\n",
    "        for i in range(num_questions + 1):\n",
    "            if state[i] == 0:  # まだ解けていない問題\n",
    "                # 遷移後の状態\n",
    "                next_state = list(state)\n",
    "                next_state[i] = 1\n",
    "                next_state = tuple(next_state)\n",
    "                node_probabilities[next_state] += probabilities[i] * node_probabilities[state]\n",
    "        \n",
    "        # budgetの計算\n",
    "        c_g = torch.ones(num_questions+1, dtype=torch.float32)\n",
    "        c_g = c_g - torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # 状態とその予測分布\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        predicted_values = model(state_tensor, c_g)  # 予測値を計算\n",
    "\n",
    "    # 評価指標の計算\n",
    "        kl = node_probabilities[state] * kl_divergence(probabilities, predicted_values.detach())\n",
    "        hd = node_probabilities[state] * hellinger_distance(probabilities, predicted_values.detach())\n",
    "        jsd = node_probabilities[state] * jensen_shannon_divergence(probabilities, predicted_values.detach())\n",
    "        # print(f\"状態: {state}, ノード分布: {node_probabilities[state]:.3g}, KL: {kl:.3g}, HD: {hd:.3g}, JSD: {jsd:.3g}\")\n",
    "        # print(f\"真の遷移確率: {probabilities}, 予測遷移確率: {predicted_values}\")\n",
    "        KL += kl\n",
    "        HD += hd\n",
    "        JSD += jsd\n",
    "\n",
    "\n",
    "    # 試行ごとの指標をCSVに追記\n",
    "    with open(results_csv, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            trial + 1,\n",
    "            KL,\n",
    "            HD,\n",
    "            JSD\n",
    "        ])\n",
    "\n",
    "    print(f\"Trial {trial + 1} completed. Results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
